Voici un fichier `README.md` complet et clair pour votre projet. Ce document dÃ©taille les Ã©tapes nÃ©cessaires pour exÃ©cuter tout le pipeline localement et dans un conteneur Docker avec HDFS.

---

# Hadoop TP3 - Collaborative Filtering

Ce projet implÃ©mente un pipeline de filtrage collaboratif distribuÃ© utilisant Hadoop. Il inclut trois jobs MapReduce qui prennent en entrÃ©e un fichier de relations utilisateur et produisent des recommandations basÃ©es sur des relations communes.

## Structure du Projet

```
.
â”œâ”€â”€ data/                           # Dossier contenant les donnÃ©es d'entrÃ©e
â”‚   â””â”€â”€ relationships/data.txt      # Fichier de relations utilisateur
â”œâ”€â”€ deploy/                         # Fichiers nÃ©cessaires pour construire l'image Docker
â”‚   â”œâ”€â”€ Dockerfile                  # Dockerfile pour configurer Hadoop et HDFS
â”‚   â”œâ”€â”€ core-site.xml               # Configuration HDFS
â”‚   â”œâ”€â”€ entrypoint.sh               # Script de dÃ©marrage du conteneur
â”‚   â””â”€â”€ ...                         # Autres fichiers de configuration
â”œâ”€â”€ jars/                           # Dossier oÃ¹ sont copiÃ©s les JARs gÃ©nÃ©rÃ©s
â”‚   â”œâ”€â”€ tpfinal-mourchid_moutuidine_job1.jar
â”‚   â”œâ”€â”€ tpfinal-mourchid_moutuidine_job2.jar
â”‚   â””â”€â”€ tpfinal-mourchid_moutuidine_job3.jar
â”œâ”€â”€ p-collaborative-filtering-job-1 # Code source et configuration du Job 1
â”œâ”€â”€ p-collaborative-filtering-job-2 # Code source et configuration du Job 2
â”œâ”€â”€ p-collaborative-filtering-job-3 # Code source et configuration du Job 3
â””â”€â”€ pom.xml                         # Configuration Maven Ã  la racine
```

## PrÃ©requis

Avant de commencer, assurez-vous d'avoir les outils suivants installÃ©s sur votre machine localeÂ :

- **Docker** : pour exÃ©cuter le conteneur Hadoop dans Docker.
- **Java (JDK 8)** : nÃ©cessaire pour compiler les jobs Hadoop.
- **Maven** : pour construire les projets Java.

---

## Ã‰tapes pour ExÃ©cuter le Projet

### 1. Cloner le RÃ©pertoire

Clonez ce dÃ©pÃ´t sur votre machineÂ :

```bash
git clone <votre-lien-du-repo>
cd hadoop-tp3
```

### 2. Compiler les Jobs Hadoop

AccÃ©dez aux sous-projets et gÃ©nÃ©rez les JARs pour chaque jobÂ :

```bash
# Compiler le Job 1
cd p-collaborative-filtering-job-1
mvn clean install
cd ..

# Compiler le Job 2
cd p-collaborative-filtering-job-2
mvn clean install
cd ..

# Compiler le Job 3
cd p-collaborative-filtering-job-3
mvn clean install
cd ..
```

### 3. Copier les JARs dans le Dossier `/jars`

Une fois les jobs compilÃ©s, copiez les fichiers `.jar` gÃ©nÃ©rÃ©s dans le dossier `jars`Â :

```bash
cp p-collaborative-filtering-job-1/target/hadoop-tp3-collaborativeFiltering-job1-1.0.jar jars/tpfinal-mourchid_moutuidine_job1.jar
cp p-collaborative-filtering-job-2/target/hadoop-tp3-collaborativeFiltering-job2-1.0.jar jars/tpfinal-mourchid_moutuidine_job2.jar
cp p-collaborative-filtering-job-3/target/hadoop-tp3-collaborativeFiltering-job3-1.0.jar jars/tpfinal-mourchid_moutuidine_job3.jar
```

### 4. Construire l'Image Docker

AccÃ©dez au dossier `deploy` et construisez l'image DockerÂ :

```bash
cd deploy
docker build -t hadoop-tp3-img .
cd ..
```

### 5. Lancer le Conteneur Docker

ExÃ©cutez un conteneur basÃ© sur l'image Docker que vous venez de crÃ©erÂ :

```bash
docker run --rm -d \
  -p 8088:8088 -p 9870:9870 -p 9864:9864 \
  -v "$(pwd)/jars:/jars" \
  -v "$(pwd)/data:/data" \
  --name hadoop-tp3-cont hadoop-tp3-img
```

### 6. AccÃ©der au Conteneur Docker

Entrez dans le conteneur Docker en tant qu'utilisateur Hadoop (`epfuser`) :

```bash
docker exec -it hadoop-tp3-cont su - epfuser
```

---

## Ã‰tapes dans le Conteneur Docker

### 1. CrÃ©er les RÃ©pertoires HDFS

CrÃ©ez les rÃ©pertoires nÃ©cessaires dans le systÃ¨me HDFSÂ :

```bash
hdfs dfs -mkdir -p /data/relationships
hdfs dfs -mkdir -p /data/output
hdfs dfs -mkdir -p /jars
```

### 2. Charger les Fichiers dans HDFS

Ajoutez le fichier d'entrÃ©e et les JARs dans HDFSÂ :

```bash
hdfs dfs -put /data/relationships/data.txt /data/relationships/
hdfs dfs -put /jars/tpfinal-mourchid_moutuidine_job*.jar /jars
```

### 3. ExÃ©cuter les Jobs Hadoop

ExÃ©cutez les trois jobs Hadoop en sÃ©quenceÂ :

```bash
# Job 1
hadoop jar /jars/tpfinal-mourchid_moutuidine_job1.jar /data/relationships/data.txt /data/output/job1

# Job 2
hadoop jar /jars/tpfinal-mourchid_moutuidine_job2.jar /data/output/job1 /data/output/job2

# Job 3
hadoop jar /jars/tpfinal-mourchid_moutuidine_job3.jar /data/output/job2 /data/output/job3
```

### 4. TÃ©lÃ©charger les RÃ©sultats Finaux

TÃ©lÃ©chargez les rÃ©sultats finaux sur votre machine locale depuis HDFSÂ :

```bash
hdfs dfs -get /data/output/job3/part-r-00000 /data/output/final_recommendations.txt
```

---

## AccÃ¨s au RÃ©sultat

Les rÃ©sultats finaux se trouvent dans le fichier en local (dans le projet)Â :

```
data/output/final_recommendations.txt
```

---

## Points Importants

- **Ports ExposÃ©s**Â :
  - **8088**Â : Interface web YARN.
  - **9870**Â : Interface web HDFS.
  - **9864**Â : Port du DataNode HDFS.
- **Chemin des donnÃ©es**Â :
  - Les donnÃ©es d'entrÃ©e doivent Ãªtre placÃ©es dans `data/relationships/data.txt`.

---

## RÃ©sultat attendu (dans `final_recommendations.txt`)Â :

```
alice   david:3
bob     eve:2, frank:2
charlie eve:3
david   alice:3, frank:2
eve     charlie:3, bob:2
frank   bob:2, david:2
```

ğŸ‰ **Le projet est opÃ©rationnelÂ !**
